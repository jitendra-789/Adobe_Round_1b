Here is a complete and concise README.md file for your Adobe_Round_1b submission, fully aligned with the required deliverables, execution expectations, and scoring criteria you just shared:

â¸»


# Adobe Round 1B â€“ Intelligent Document Section Extractor

## ğŸ“ Overview

This project addresses **Challenge 1B** of the Adobe Document Intelligence task. The goal is to automatically extract the most **relevant sections and sub-sections** from a set of input documents (PDFs) based on a provided **persona** and **job-to-be-done (JTBD)** description. The output is a structured JSON containing metadata, ranked sections, and refined sub-section analysis.

---

## ğŸ“‚ Directory Structure

Adobe_Round_1b/
â”œâ”€â”€ input/                      # Folder for input PDFs and persona
â”‚   â”œâ”€â”€ *.pdf
â”‚   â”œâ”€â”€ persona.json
â”‚   â””â”€â”€ job_description.json
â”œâ”€â”€ output/
â”‚   â””â”€â”€ challenge1b_output.json
â”œâ”€â”€ models/                    # Pre-downloaded FLAN-T5/MT5 model files
â”œâ”€â”€ main.py                    # Entry-point script for processing
â”œâ”€â”€ utils/                     # Helper modules
â”‚   â””â”€â”€ *.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ approach_explanation.md
â””â”€â”€ README.md

---

## âš™ï¸ Execution Instructions (Docker)

### ğŸ”§ 1. Build Docker Image

```bash
docker build -t adobe_round_1b .

ğŸš€ 2. Run the Container

Assuming your inputs are in input/ folder:

docker run --rm \
  -v $(pwd)/input:/app/input \
  -v $(pwd)/output:/app/output \
  adobe_round_1b

This will generate challenge1b_output.json in the output/ folder.

â¸»

ğŸ“¤ Output Format

The generated JSON includes the following:

1. Metadata

{
  "input_documents": [...],
  "persona": "...",
  "job_to_be_done": "...",
  "processing_timestamp": "..."
}

2. Extracted Sections

[
  {
    "document": "doc1.pdf",
    "page_number": 3,
    "section_title": "Experience Overview",
    "importance_rank": 1
  },
  ...
]

3. Sub-section Analysis

[
  {
    "document": "doc1.pdf",
    "refined_text": "Detailed sub-section relevant to job",
    "page_number": 3
  },
  ...
]

âœ… Output JSON is aligned with challenge1b_output.json sample format.

â¸»

ğŸ“¦ Deliverables
	â€¢	approach_explanation.md: Methodology and model design (300â€“500 words).
	â€¢	Dockerfile: Containerized setup for offline, CPU-only execution.
	â€¢	README.md: This file with setup, usage, and references.
	â€¢	challenge1b_output.json: Output from sample test documents (placed in /output).

â¸»

âš–ï¸ Evaluation Criteria Mapping

Criteria	Points	Project Alignment
Section Relevance	60	Uses embedding + persona/job scoring with ranking logic
Sub-Section Relevance	40	Granular sub-section analysis using semantic refinement
Execution Constraints	â€”	âœ… CPU-onlyâœ… Model â‰¤ 1GBâœ… Offline inference
Output Format	â€”	âœ… Structured JSON output


â¸»

ğŸ“„ Refer to
	â€¢	approach_explanation.md for details on methodology and implementation logic.

â¸»

â— Notes
	â€¢	Please ensure model files are placed in models/ before building the Docker image.
	â€¢	Internet access is not required for Docker runtime.

---

Let me know if you'd like a version tailored to a different model or structure (e.g., if `main.py` takes args like `--input` and `--output`).